// Vercel Serverless Function for Gemini Conversation API
import { GoogleGenerativeAI, SchemaType } from "@google/generative-ai";
import { handleCORS, setSecurityHeaders, checkRateLimit, validateTopicInput } from './_middleware.js';

export default async function handler(req, res) {
    // Apply security headers
    setSecurityHeaders(res);

    // Handle CORS
    const corsAllowed = handleCORS(req, res);

    if (req.method === 'OPTIONS') {
        return res.status(200).end();
    }

    // Reject requests from non-allowed origins
    if (!corsAllowed) {
        return res.status(403).json({ error: 'Origin not allowed' });
    }

    // Check rate limit
    const rateLimit = checkRateLimit(req);
    res.setHeader('X-RateLimit-Limit', '10');
    res.setHeader('X-RateLimit-Remaining', rateLimit.remaining.toString());
    res.setHeader('X-RateLimit-Reset', new Date(Date.now() + rateLimit.resetIn).toISOString());

    if (!rateLimit.allowed) {
        return res.status(429).json({
            error: 'Too many requests. Please try again later.',
            retryAfter: Math.ceil(rateLimit.resetIn / 1000)
        });
    }

    if (req.method !== 'POST' && req.method !== 'GET') {
        return res.status(405).json({ error: 'Method not allowed' });
    }

    try {
        // Environment variable check
        const apiKey = process.env.GEMINI_API_KEY;

        if (!apiKey || apiKey === 'your_gemini_api_key_here') {
            console.error('âŒ GEMINI_API_KEY not configured');
            return res.status(500).json({ error: 'GEMINI_API_KEY environment variable is not set in Vercel.' });
        }

        const { topic: bodyTopic } = (req.body || {});
        const queryTopic = (req.query && req.query.topic) ? String(req.query.topic) : '';
        const rawTopic = req.method === 'GET' ? queryTopic : bodyTopic;

        // Validate and sanitize input
        const validation = validateTopicInput(rawTopic);
        if (!validation.valid) {
            return res.status(400).json({ error: validation.error });
        }

        const topic = validation.sanitized;

        console.log('ğŸ¤– Vercel Gemini Conversation Request:', { 
            method: req.method, 
            topic: topic.substring(0, 50) + '...'
        });

        const genAI = new GoogleGenerativeAI(apiKey);
        const model = genAI.getGenerativeModel({ model: "gemini-2.0-flash" });
        
        // Retry configuration for handling API overload
        const maxRetries = 5;
        const baseDelay = 2000; // 2 seconds
        
        const conversationSchema = {
            type: SchemaType.ARRAY,
            items: {
                type: SchemaType.OBJECT,
                properties: {
                    speaker: { type: SchemaType.STRING, description: "è©±è€…å (ä¾‹: A, B, åº—å“¡)" },
                    thai: { type: SchemaType.STRING, description: "ã‚¿ã‚¤èªã®ã‚»ãƒªãƒ•" },
                    pronunciation: { type: SchemaType.STRING, description: "Paiboon+æ–¹å¼ã®ãƒ­ãƒ¼ãƒå­—ç™ºéŸ³è¡¨è¨˜" },
                    japanese: { type: SchemaType.STRING, description: "æ—¥æœ¬èªè¨³" },
                    words: {
                        type: SchemaType.ARRAY,
                        description: "ã‚»ãƒªãƒ•ã‚’æ§‹æˆã™ã‚‹å˜èªã®ãƒªã‚¹ãƒˆ",
                        items: {
                            type: SchemaType.OBJECT,
                            properties: {
                                thai: { type: SchemaType.STRING, description: "å˜èªã®ã‚¿ã‚¤èªè¡¨è¨˜" },
                                pronunciation: { type: SchemaType.STRING, description: "å˜èªã®Paiboon+æ–¹å¼ãƒ­ãƒ¼ãƒå­—ç™ºéŸ³è¡¨è¨˜" },
                                japanese: { type: SchemaType.STRING, description: "å˜èªã®æ—¥æœ¬èªè¨³" },
                            },
                            required: ["thai", "pronunciation", "japanese"],
                        },
                    },
                    grammarPoint: {
                        type: SchemaType.OBJECT,
                        description: "ã“ã®ã‚»ãƒªãƒ•ã«å«ã¾ã‚Œã‚‹é‡è¦ãªæ–‡æ³•ãƒã‚¤ãƒ³ãƒˆã®è§£èª¬ã€‚è©²å½“ã™ã‚‹å ´åˆã®ã¿ç”Ÿæˆã™ã‚‹ã€‚",
                        properties: {
                            title: { type: SchemaType.STRING, description: "æ–‡æ³•ãƒã‚¤ãƒ³ãƒˆã®ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆä¾‹ï¼šæ–‡æœ«è©ã€Œã€œã§ã™/ã¾ã™ã€ï¼‰" },
                            explanation: { type: SchemaType.STRING, description: "æ–‡æ³•ãƒ«ãƒ¼ãƒ«ã®åˆ†ã‹ã‚Šã‚„ã™ã„è§£èª¬" },
                            examples: {
                                type: SchemaType.ARRAY,
                                description: "æ–‡æ³•ã‚’ä½¿ã£ãŸä¾‹æ–‡ã®ãƒªã‚¹ãƒˆ",
                                items: {
                                    type: SchemaType.OBJECT,
                                    properties: {
                                        thai: { type: SchemaType.STRING, description: "ä¾‹æ–‡ã®ã‚¿ã‚¤èªè¡¨è¨˜" },
                                        pronunciation: { type: SchemaType.STRING, description: "ä¾‹æ–‡ã®Paiboon+æ–¹å¼ãƒ­ãƒ¼ãƒå­—ç™ºéŸ³è¡¨è¨˜" },
                                        japanese: { type: SchemaType.STRING, description: "ä¾‹æ–‡ã®æ—¥æœ¬èªè¨³" },
                                    },
                                    required: ["thai", "pronunciation", "japanese"],
                                }
                            }
                        },
                        required: ["title", "explanation", "examples"]
                    }
                },
                required: ["speaker", "thai", "pronunciation", "japanese", "words"],
            },
        };

        // Retry function with exponential backoff
        async function generateWithRetry(retryCount = 0) {
            try {
                const response = await model.generateContent({
                    contents: [{
                        role: "user",
                        parts: [{
                            text: `ãƒˆãƒ”ãƒƒã‚¯: ã€Œ${topic}ã€

ã‚¿ã‚¤äººãƒã‚¤ãƒ†ã‚£ãƒ–ã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼ãŒå®Ÿéš›ã«ä½¿ã†ã€è‡ªç„¶ã§ç”Ÿãç”Ÿãã¨ã—ãŸæ—¥å¸¸ä¼šè©±ï¼ˆ4-6ã‚¿ãƒ¼ãƒ³ï¼‰ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ã€ãƒã‚¤ãƒ†ã‚£ãƒ–ã‚‰ã—ã•ã®è¦ä»¶ã€‘â˜…é‡è¦â˜…
1. å£èªè¡¨ç¾ã‚’ç©æ¥µçš„ã«ä½¿ç”¨:
   - ã€Œà¹€à¸›à¹‡à¸™à¹„à¸‡à¸šà¹‰à¸²à¸‡ã€(èª¿å­ã©ã†ï¼Ÿ) â† ã€Œà¸ªà¸šà¸²à¸¢à¸”à¸µà¹„à¸«à¸¡ã€ã‚ˆã‚Šè‡ªç„¶
   - ã€Œà¸­à¸£à¹ˆà¸­à¸¢à¸¡à¸²à¸ã€â†’ã€Œà¸­à¸£à¹ˆà¸­à¸¢à¹‚à¸„à¸•à¸£ã€ã€Œà¸­à¸£à¹ˆà¸­à¸¢à¸ªà¸¸à¸”à¹†ã€ï¼ˆè¶…ãŠã„ã—ã„ï¼‰
   - ã€Œà¹„à¸¡à¹ˆà¹€à¸›à¹‡à¸™à¹„à¸£ã€â†’ã€Œà¹„à¸¡à¹ˆà¹€à¸›à¹‡à¸™à¹„à¸£à¸­à¹ˆà¸°ã€ã€Œà¸Šà¹ˆà¸²à¸‡à¸¡à¸±à¸™à¹€à¸–à¸­à¸°ã€

2. æ–‡æœ«åŠ©è©ã‚’è‡ªç„¶ã«ä½¿ã„åˆ†ã‘:
   - ã€Œà¸™à¸°ã€ã€Œà¸™à¹ˆà¸°ã€ã€Œà¸ªà¸´ã€ã€Œà¸¥à¹ˆà¸°ã€ã€Œà¹€à¸¥à¸¢ã€ã€Œà¸­à¹ˆà¸°ã€ã€Œà¸ˆà¹‰à¸²ã€ã€Œà¸ˆà¹Šà¸°ã€
   - à¸„à¸£à¸±à¸š/à¸„à¹ˆà¸° ã¯å…¨æ–‡ã«ä»˜ã‘ãªã„ï¼ˆè¦ªã—ã„é–“æŸ„ã§ã¯çœç•¥ãŒè‡ªç„¶ï¼‰
   - å¥³æ€§ã®æŸ”ã‚‰ã‹ã„è¡¨ç¾: ã€Œã€œà¸ˆà¹‰à¸²ã€ã€Œã€œà¸™à¸°à¸„à¸°ã€

3. ãƒ•ã‚£ãƒ©ãƒ¼ãƒ»é–“æŠ•è©ã‚’å«ã‚ã‚‹:
   - ã€Œà¹€à¸­à¹ˆà¸­ã€(ãˆãƒ¼ã¨) ã€Œà¸­à¹ˆà¸²ã€(ã‚ãƒ¼) ã€Œà¹€à¸­à¸²ã€(ã˜ã‚ƒã‚)
   - ã€Œà¸ˆà¸£à¸´à¸‡à¹†ã€(ãƒã‚¸ã§) ã€Œà¹€à¸«à¸£à¸­ã€(ãã†ãªã®ï¼Ÿ) ã€Œà¸­à¸·à¸¡ã€(ã†ãƒ¼ã‚“)

4. è‡ªç„¶ãªåå¿œãƒ‘ã‚¿ãƒ¼ãƒ³:
   - ã€Œà¹‚à¸­à¹€à¸„ã€ã€Œà¹„à¸”à¹‰à¹€à¸¥à¸¢ã€ã€Œà¸”à¸µà¹€à¸¥à¸¢ã€ï¼ˆOKç³»ï¼‰
   - ã€Œà¹ƒà¸Šà¹ˆà¹†ã€ã€Œà¹ƒà¸Šà¹ˆà¹€à¸¥à¸¢ã€ï¼ˆãã†ãã†ï¼‰
   - ã€Œà¹€à¸‚à¹‰à¸²à¹ƒà¸ˆà¹à¸¥à¹‰à¸§ã€â†’ã€Œà¹‚à¸­à¹€à¸„ à¹€à¸‚à¹‰à¸²à¹ƒà¸ˆã€

ã€å­¦ç¿’è€…ã¸ã®é…æ…®ã€‘
- å£èªè¡¨ç¾ã‚’ä½¿ã£ãŸå ´åˆã¯ã€grammarPointã§ä¸å¯§å½¢ã¨ã®é•ã„ã‚’è§£èª¬
- åˆå¿ƒè€…ã§ã‚‚ç†è§£ã§ãã‚‹ã‚ˆã†ã€æ¥µç«¯ãªã‚¹ãƒ©ãƒ³ã‚°ã¯é¿ã‘ã‚‹
- ç™ºéŸ³è¡¨è¨˜ã¯Paiboon+æ–¹å¼ã‚’å³å®ˆ

ã€å¿…é ˆé …ç›®ã€‘
1. äºŒäººã®è©±è€…é–“ï¼ˆAã¨Bï¼‰ã®è‡ªç„¶ãªä¼šè©±
2. Paiboon+æ–¹å¼ã®ãƒ­ãƒ¼ãƒå­—ç™ºéŸ³è¡¨è¨˜
3. å„ã‚»ãƒªãƒ•ã®å˜èªåˆ†å‰²ï¼ˆåŠ©è©ã‚‚ç‹¬ç«‹ã—ã¦åˆ†å‰²ï¼‰
4. å£èªè¡¨ç¾ãƒ»ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«è¡¨ç¾ã®grammarPointè§£èª¬

æŒ‡å®šã•ã‚ŒãŸJSONã‚¹ã‚­ãƒ¼ãƒã«å¾“ã£ã¦ã€æœ‰åŠ¹ãªJSONé…åˆ—ã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚`
                        }]
                    }],
                    generationConfig: {
                        responseMimeType: "application/json", // JSONãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹åŒ–
                        responseSchema: conversationSchema, // ã‚¹ã‚­ãƒ¼ãƒã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
                        temperature: 0.75, // ã‚ˆã‚Šè‡ªç„¶ãªãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã®ãŸã‚å°‘ã—é«˜ã‚ã«
                        topP: 0.92,
                        topK: 50,
                        maxOutputTokens: 8192 // ã‚ˆã‚Šä½™è£•ã‚’æŒã£ãŸè¨­å®š
                    },
                });
                return response;
            } catch (error) {
                // Check if it's a 503 Service Unavailable error (model overloaded)
                const isOverloaded = error?.message?.includes('503') || 
                                   error?.message?.includes('overloaded') ||
                                   error?.message?.includes('Service Unavailable');
                
                if (isOverloaded && retryCount < maxRetries) {
                    const delay = baseDelay * Math.pow(2, retryCount); // Exponential backoff
                    console.warn(`ğŸ”„ API overloaded, retrying in ${delay}ms (attempt ${retryCount + 1}/${maxRetries})`);
                    
                    await new Promise(resolve => setTimeout(resolve, delay));
                    return generateWithRetry(retryCount + 1);
                }
                
                throw error; // Re-throw if not retryable or max retries reached
            }
        }

        const response = await generateWithRetry();

        // JSONãƒ¢ãƒ¼ãƒ‰ã§ã¯ã€ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã¯ç›´æ¥ãƒ‘ãƒ¼ã‚¹å¯èƒ½ãªJSONæ–‡å­—åˆ—ã«ãªã‚‹
        const text = response.response.text();

        if (!text) {
            throw new Error('Empty response from Gemini API');
        }

        console.log('ğŸ¤– Raw JSON response length:', text.length);
        console.log('ğŸ¤– Raw JSON response (first 500 chars):', text.substring(0, 500));

        let conversation;
        try {
            // JSONãƒ¢ãƒ¼ãƒ‰ã‹ã‚‰ã®å¿œç­”ã¯ç›´æ¥ãƒ‘ãƒ¼ã‚¹ã§ãã‚‹
            conversation = JSON.parse(text);
        } catch (parseError) {
            console.error('âŒ JSON Parse Error:', {
                error: parseError.message,
                position: parseError.message.match(/position (\d+)/)?.[1],
                contextAround: text.substring(
                    Math.max(0, parseInt(parseError.message.match(/position (\d+)/)?.[1] || 0) - 100),
                    Math.min(text.length, parseInt(parseError.message.match(/position (\d+)/)?.[1] || 0) + 100)
                ),
                fullText: text
            });
            throw parseError;
        }

        res.status(200).json({ conversation });
    } catch (error) {
        console.error('âŒ Vercel Gemini conversation error:', {
            message: error?.message,
            type: typeof error,
            stack: error?.stack,
            errorObject: JSON.stringify(error, null, 2)
        });

        // Determine appropriate error response based on error type
        let statusCode = 500;
        let errorMessage = 'Failed to generate conversation';

        if (error?.message?.includes('503') || error?.message?.includes('overloaded')) {
            statusCode = 503;
            errorMessage = 'AI service is currently overloaded. Please try again in a few moments.';
        } else if (error?.message?.includes('API key') || error?.message?.includes('API_KEY')) {
            statusCode = 401;
            errorMessage = 'API authentication failed';
        } else if (error?.message?.includes('quota')) {
            statusCode = 429;
            errorMessage = 'API usage limit exceeded. Please try again later.';
        } else if (error?.message?.includes('JSON') || error?.message?.includes('parse')) {
            statusCode = 502;
            errorMessage = 'Failed to parse AI response. Please try again.';
        } else if (error?.message?.includes('model') || error?.message?.includes('not found')) {
            statusCode = 400;
            errorMessage = 'Invalid model configuration. Please contact support.';
        }

        res.status(statusCode).json({
            error: errorMessage,
            details: error?.message || 'Unknown error',
            type: error?.name || 'Unknown error',
            retryAfter: statusCode === 503 ? 30 : undefined // Suggest retry after 30 seconds for overload
        });
    }
}
